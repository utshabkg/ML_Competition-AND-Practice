{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np                               # vectors and matrices\nimport pandas as pd                              # tables and data manipulations\nimport matplotlib.pyplot as plt                  # plots\nimport seaborn as sns                            # more plots\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\nimport warnings                                  # `do not disturbe` mode\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-13T07:33:22.152711Z","iopub.execute_input":"2021-12-13T07:33:22.153005Z","iopub.status.idle":"2021-12-13T07:33:22.164962Z","shell.execute_reply.started":"2021-12-13T07:33:22.152974Z","shell.execute_reply":"2021-12-13T07:33:22.164340Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"let’s use some real mobile game data on hourly ads watched by players and daily in-game currency spent.","metadata":{}},{"cell_type":"code","source":"ads = pd.read_csv('../input/mlcourse/ads.csv', index_col=['Time'], parse_dates=['Time'])\ncurrency = pd.read_csv('../input/mlcourse/currency.csv', index_col=['Time'], parse_dates=['Time'])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.166478Z","iopub.execute_input":"2021-12-13T07:33:22.167265Z","iopub.status.idle":"2021-12-13T07:33:22.235729Z","shell.execute_reply.started":"2021-12-13T07:33:22.167234Z","shell.execute_reply":"2021-12-13T07:33:22.234977Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.237413Z","iopub.execute_input":"2021-12-13T07:33:22.238492Z","iopub.status.idle":"2021-12-13T07:33:22.250001Z","shell.execute_reply.started":"2021-12-13T07:33:22.238435Z","shell.execute_reply":"2021-12-13T07:33:22.249042Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(ads.Ads)\nplt.title('Ads watched (hourly data)')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.252431Z","iopub.execute_input":"2021-12-13T07:33:22.253025Z","iopub.status.idle":"2021-12-13T07:33:22.532891Z","shell.execute_reply.started":"2021-12-13T07:33:22.252972Z","shell.execute_reply":"2021-12-13T07:33:22.531968Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"currency.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.534411Z","iopub.execute_input":"2021-12-13T07:33:22.534897Z","iopub.status.idle":"2021-12-13T07:33:22.546265Z","shell.execute_reply.started":"2021-12-13T07:33:22.534848Z","shell.execute_reply":"2021-12-13T07:33:22.545348Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(currency.GEMS_GEMS_SPENT)\nplt.title('In-game currency spent (daily data)')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.547793Z","iopub.execute_input":"2021-12-13T07:33:22.548382Z","iopub.status.idle":"2021-12-13T07:33:22.840515Z","shell.execute_reply.started":"2021-12-13T07:33:22.548336Z","shell.execute_reply":"2021-12-13T07:33:22.839749Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# 1. Move, smoothe, evaluate","metadata":{}},{"cell_type":"markdown","source":"### Moving Average","metadata":{}},{"cell_type":"code","source":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(ads, 24) # prediction for the last observed day (past 24 hours)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.842151Z","iopub.execute_input":"2021-12-13T07:33:22.843106Z","iopub.status.idle":"2021-12-13T07:33:22.851988Z","shell.execute_reply.started":"2021-12-13T07:33:22.843059Z","shell.execute_reply":"2021-12-13T07:33:22.851187Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately we can’t make this prediction long-term — to get one for the next step we need the previous value to be actually observed. But moving average has another use case — smoothing of the original time series to indicate trends. \n\nPandas has an implementation available DataFrame.rolling(window).mean(). The wider the window - the smoother will be the trend. In the case of the very noisy data, which can be very often encountered in finance, this procedure can help to detect common patterns.","metadata":{}},{"cell_type":"code","source":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.853810Z","iopub.execute_input":"2021-12-13T07:33:22.854728Z","iopub.status.idle":"2021-12-13T07:33:22.868965Z","shell.execute_reply.started":"2021-12-13T07:33:22.854684Z","shell.execute_reply":"2021-12-13T07:33:22.868158Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Smoothing by last 4 hours\nplotMovingAverage(ads, 4)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:22.871883Z","iopub.execute_input":"2021-12-13T07:33:22.872651Z","iopub.status.idle":"2021-12-13T07:33:23.350310Z","shell.execute_reply.started":"2021-12-13T07:33:22.872608Z","shell.execute_reply":"2021-12-13T07:33:23.349391Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Smoothing by last 12 hours\nplotMovingAverage(ads, 12)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:23.351531Z","iopub.execute_input":"2021-12-13T07:33:23.351738Z","iopub.status.idle":"2021-12-13T07:33:23.646832Z","shell.execute_reply.started":"2021-12-13T07:33:23.351711Z","shell.execute_reply":"2021-12-13T07:33:23.645955Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Smoothing by 24 hours — we get daily trend\nplotMovingAverage(ads, 24)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:23.648053Z","iopub.execute_input":"2021-12-13T07:33:23.648301Z","iopub.status.idle":"2021-12-13T07:33:23.956797Z","shell.execute_reply.started":"2021-12-13T07:33:23.648250Z","shell.execute_reply":"2021-12-13T07:33:23.955918Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"As you can see, applying daily smoothing on hour data allowed us to clearly see the dynamics of ads watched. During the weekends the values are higher (weekends — time to play) and weekdays are generally lower.\n\nWe can also plot confidence intervals for our smoothed values","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(ads, 4, plot_intervals=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:23.957970Z","iopub.execute_input":"2021-12-13T07:33:23.958210Z","iopub.status.idle":"2021-12-13T07:33:24.292740Z","shell.execute_reply.started":"2021-12-13T07:33:23.958174Z","shell.execute_reply":"2021-12-13T07:33:24.291225Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"And now let’s create a simple anomaly detection system with the help of the moving average. Unfortunately, in this particular series everything is more or less normal, so we’ll intentionally make one of the values abnormal in the dataframe ads_anomaly","metadata":{}},{"cell_type":"code","source":"ads_anomaly = ads.copy()\nads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2 # say we have 80% drop of ads","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:24.294249Z","iopub.execute_input":"2021-12-13T07:33:24.294711Z","iopub.status.idle":"2021-12-13T07:33:24.301170Z","shell.execute_reply.started":"2021-12-13T07:33:24.294673Z","shell.execute_reply":"2021-12-13T07:33:24.300349Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Let’s see, if this simple method can catch the anomaly","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:24.302771Z","iopub.execute_input":"2021-12-13T07:33:24.303211Z","iopub.status.idle":"2021-12-13T07:33:24.667287Z","shell.execute_reply.started":"2021-12-13T07:33:24.303172Z","shell.execute_reply":"2021-12-13T07:33:24.666395Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Neat! What about the second series (with weekly smoothing)?","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(currency, 7, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:24.668571Z","iopub.execute_input":"2021-12-13T07:33:24.668786Z","iopub.status.idle":"2021-12-13T07:33:25.038840Z","shell.execute_reply.started":"2021-12-13T07:33:24.668759Z","shell.execute_reply":"2021-12-13T07:33:25.038038Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Oh no! Here is the downside of our simple approach — it did not catch monthly seasonality in our data and marked almost all 30-day peaks as an anomaly. If you don’t want to have that many false alarms — it’s best to consider more complex models.\n\nWeighted average is a simple modification of the moving average, inside of which observations have different weights summing up to one, usually more recent observations have greater weight.","metadata":{}},{"cell_type":"markdown","source":"### Weighted Average","metadata":{}},{"cell_type":"code","source":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)\n  \nweighted_average(ads, [0.6, 0.3, 0.1])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:25.040216Z","iopub.execute_input":"2021-12-13T07:33:25.040479Z","iopub.status.idle":"2021-12-13T07:33:25.051029Z","shell.execute_reply.started":"2021-12-13T07:33:25.040448Z","shell.execute_reply":"2021-12-13T07:33:25.050058Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Exponential smoothing","metadata":{}},{"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n    \ndef plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:25.052634Z","iopub.execute_input":"2021-12-13T07:33:25.052850Z","iopub.status.idle":"2021-12-13T07:33:25.068654Z","shell.execute_reply.started":"2021-12-13T07:33:25.052823Z","shell.execute_reply":"2021-12-13T07:33:25.067757Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"plotExponentialSmoothing(ads.Ads, [0.3, 0.05])\nplotExponentialSmoothing(currency.GEMS_GEMS_SPENT, [0.3, 0.05])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:25.070180Z","iopub.execute_input":"2021-12-13T07:33:25.070466Z","iopub.status.idle":"2021-12-13T07:33:25.614013Z","shell.execute_reply.started":"2021-12-13T07:33:25.070432Z","shell.execute_reply":"2021-12-13T07:33:25.613376Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### Double exponential smoothing","metadata":{}},{"cell_type":"code","source":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:25.615234Z","iopub.execute_input":"2021-12-13T07:33:25.615492Z","iopub.status.idle":"2021-12-13T07:33:25.626995Z","shell.execute_reply.started":"2021-12-13T07:33:25.615462Z","shell.execute_reply":"2021-12-13T07:33:25.626184Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"plotDoubleExponentialSmoothing(ads.Ads, alphas=[0.9, 0.02], betas=[0.9, 0.02])\nplotDoubleExponentialSmoothing(currency.GEMS_GEMS_SPENT, alphas=[0.9, 0.02], betas=[0.9, 0.02])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:25.628329Z","iopub.execute_input":"2021-12-13T07:33:25.628580Z","iopub.status.idle":"2021-12-13T07:33:26.325982Z","shell.execute_reply.started":"2021-12-13T07:33:25.628549Z","shell.execute_reply":"2021-12-13T07:33:26.325125Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Triple exponential smoothing a.k.a. Holt-Winters","metadata":{}},{"cell_type":"code","source":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:26.327344Z","iopub.execute_input":"2021-12-13T07:33:26.327593Z","iopub.status.idle":"2021-12-13T07:33:26.352057Z","shell.execute_reply.started":"2021-12-13T07:33:26.327562Z","shell.execute_reply":"2021-12-13T07:33:26.351191Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### Time series cross validation\nNow, knowing how to set cross-validation, we will find optimal parameters for the Holt-Winters model, recall that we have daily seasonality in ads, hence the slen=24 parameter","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:26.353292Z","iopub.execute_input":"2021-12-13T07:33:26.353685Z","iopub.status.idle":"2021-12-13T07:33:26.370724Z","shell.execute_reply.started":"2021-12-13T07:33:26.353640Z","shell.execute_reply":"2021-12-13T07:33:26.369814Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"In the Holt-Winters model, as well as in the other models of exponential smoothing, there’s a constraint on how big smoothing parameters could be, each of them is in the range from 0 to 1, therefore to minimize loss function we have to choose an algorithm that supports constraints on model parameters, in our case — Truncated Newton conjugate gradient.","metadata":{}},{"cell_type":"code","source":"%%time\ndata = ads.Ads[:-20] # leave some data for testing\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_squared_log_error), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 50 hours\nmodel = HoltWinters(data, slen = 24, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 50, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:26.374100Z","iopub.execute_input":"2021-12-13T07:33:26.374361Z","iopub.status.idle":"2021-12-13T07:33:28.634110Z","shell.execute_reply.started":"2021-12-13T07:33:26.374328Z","shell.execute_reply":"2021-12-13T07:33:28.633037Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);\n    \nplotHoltWinters(ads.Ads)\nplotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:28.635708Z","iopub.execute_input":"2021-12-13T07:33:28.636035Z","iopub.status.idle":"2021-12-13T07:33:29.275758Z","shell.execute_reply.started":"2021-12-13T07:33:28.635990Z","shell.execute_reply":"2021-12-13T07:33:29.274983Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Judging by the chart, our model was able to successfully approximate the initial time series, catching daily seasonality, overall downwards trend and even some anomalies. If you take a look at the modeled deviation, you can clearly see that the model reacts quite sharply to the changes in the structure of the series but then quickly returns deviation to the normal values, “forgetting” the past. This feature of the model allows us to quickly build anomaly detection systems even for quite noisy series without spending too much time and money on preparing data and training the model.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:29.277033Z","iopub.execute_input":"2021-12-13T07:33:29.277624Z","iopub.status.idle":"2021-12-13T07:33:29.471405Z","shell.execute_reply.started":"2021-12-13T07:33:29.277585Z","shell.execute_reply":"2021-12-13T07:33:29.470630Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"We’ll apply the same algorithm for the second series which, as we know, has trend and 30-day seasonality.","metadata":{}},{"cell_type":"code","source":"%%time\ndata = currency.GEMS_GEMS_SPENT[:-50] \nslen = 30 # 30-day seasonality\n\nx = [0, 0, 0] \n\nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_absolute_percentage_error, slen), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\nmodel = HoltWinters(data, slen = slen, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 100, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:29.472543Z","iopub.execute_input":"2021-12-13T07:33:29.473226Z","iopub.status.idle":"2021-12-13T07:33:33.223245Z","shell.execute_reply.started":"2021-12-13T07:33:29.473187Z","shell.execute_reply":"2021-12-13T07:33:33.222358Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"plotHoltWinters(currency.GEMS_GEMS_SPENT)\nplotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:33.224658Z","iopub.execute_input":"2021-12-13T07:33:33.224906Z","iopub.status.idle":"2021-12-13T07:33:33.881731Z","shell.execute_reply.started":"2021-12-13T07:33:33.224874Z","shell.execute_reply":"2021-12-13T07:33:33.880912Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Looks quite adequate, model has caught both upwards trend and seasonal spikes and overall fits our values nicely\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T07:33:33.883418Z","iopub.execute_input":"2021-12-13T07:33:33.883649Z","iopub.status.idle":"2021-12-13T07:33:34.120738Z","shell.execute_reply.started":"2021-12-13T07:33:33.883619Z","shell.execute_reply":"2021-12-13T07:33:34.120174Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# 2. Econometric approach","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}