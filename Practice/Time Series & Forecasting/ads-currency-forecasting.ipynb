{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np                               # vectors and matrices\nimport pandas as pd                              # tables and data manipulations\nimport matplotlib.pyplot as plt                  # plots\nimport seaborn as sns                            # more plots\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\nimport warnings                                  # `do not disturbe` mode\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T14:01:19.457871Z","iopub.execute_input":"2021-12-15T14:01:19.458457Z","iopub.status.idle":"2021-12-15T14:01:21.334343Z","shell.execute_reply.started":"2021-12-15T14:01:19.458355Z","shell.execute_reply":"2021-12-15T14:01:21.333519Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"let’s use some real mobile game data on hourly ads watched by players and daily in-game currency spent.","metadata":{}},{"cell_type":"code","source":"ads = pd.read_csv('../input/mlcourse/ads.csv', index_col=['Time'], parse_dates=['Time'])\ncurrency = pd.read_csv('../input/mlcourse/currency.csv', index_col=['Time'], parse_dates=['Time'])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:21.336610Z","iopub.execute_input":"2021-12-15T14:01:21.336936Z","iopub.status.idle":"2021-12-15T14:01:21.399424Z","shell.execute_reply.started":"2021-12-15T14:01:21.336891Z","shell.execute_reply":"2021-12-15T14:01:21.398593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:21.400572Z","iopub.execute_input":"2021-12-15T14:01:21.400801Z","iopub.status.idle":"2021-12-15T14:01:21.416679Z","shell.execute_reply.started":"2021-12-15T14:01:21.400774Z","shell.execute_reply":"2021-12-15T14:01:21.415895Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(ads.Ads)\nplt.title('Ads watched (hourly data)')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:21.418073Z","iopub.execute_input":"2021-12-15T14:01:21.418533Z","iopub.status.idle":"2021-12-15T14:01:21.717871Z","shell.execute_reply.started":"2021-12-15T14:01:21.418492Z","shell.execute_reply":"2021-12-15T14:01:21.716967Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"currency.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:21.720163Z","iopub.execute_input":"2021-12-15T14:01:21.720420Z","iopub.status.idle":"2021-12-15T14:01:21.729952Z","shell.execute_reply.started":"2021-12-15T14:01:21.720390Z","shell.execute_reply":"2021-12-15T14:01:21.729063Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(currency.GEMS_GEMS_SPENT)\nplt.title('In-game currency spent (daily data)')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:21.731616Z","iopub.execute_input":"2021-12-15T14:01:21.732179Z","iopub.status.idle":"2021-12-15T14:01:22.016137Z","shell.execute_reply.started":"2021-12-15T14:01:21.732083Z","shell.execute_reply":"2021-12-15T14:01:22.015130Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 1. Move, smoothe, evaluate","metadata":{}},{"cell_type":"markdown","source":"### Moving Average","metadata":{}},{"cell_type":"code","source":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(ads, 24) # prediction for the last observed day (past 24 hours)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:22.017544Z","iopub.execute_input":"2021-12-15T14:01:22.017922Z","iopub.status.idle":"2021-12-15T14:01:22.026310Z","shell.execute_reply.started":"2021-12-15T14:01:22.017878Z","shell.execute_reply":"2021-12-15T14:01:22.025448Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately we can’t make this prediction long-term — to get one for the next step we need the previous value to be actually observed. But moving average has another use case — smoothing of the original time series to indicate trends. \n\nPandas has an implementation available DataFrame.rolling(window).mean(). The wider the window - the smoother will be the trend. In the case of the very noisy data, which can be very often encountered in finance, this procedure can help to detect common patterns.","metadata":{}},{"cell_type":"code","source":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:22.027933Z","iopub.execute_input":"2021-12-15T14:01:22.028252Z","iopub.status.idle":"2021-12-15T14:01:22.041004Z","shell.execute_reply.started":"2021-12-15T14:01:22.028214Z","shell.execute_reply":"2021-12-15T14:01:22.040383Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Smoothing by last 4 hours\nplotMovingAverage(ads, 4)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:22.042063Z","iopub.execute_input":"2021-12-15T14:01:22.042866Z","iopub.status.idle":"2021-12-15T14:01:22.357621Z","shell.execute_reply.started":"2021-12-15T14:01:22.042817Z","shell.execute_reply":"2021-12-15T14:01:22.357082Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Smoothing by last 12 hours\nplotMovingAverage(ads, 12)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:22.358693Z","iopub.execute_input":"2021-12-15T14:01:22.359161Z","iopub.status.idle":"2021-12-15T14:01:22.651815Z","shell.execute_reply.started":"2021-12-15T14:01:22.359125Z","shell.execute_reply":"2021-12-15T14:01:22.650949Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Smoothing by 24 hours — we get daily trend\nplotMovingAverage(ads, 24)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:22.653105Z","iopub.execute_input":"2021-12-15T14:01:22.653373Z","iopub.status.idle":"2021-12-15T14:01:23.072546Z","shell.execute_reply.started":"2021-12-15T14:01:22.653342Z","shell.execute_reply":"2021-12-15T14:01:23.071750Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"As you can see, applying daily smoothing on hour data allowed us to clearly see the dynamics of ads watched. During the weekends the values are higher (weekends — time to play) and weekdays are generally lower.\n\nWe can also plot confidence intervals for our smoothed values","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(ads, 4, plot_intervals=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:23.074167Z","iopub.execute_input":"2021-12-15T14:01:23.074702Z","iopub.status.idle":"2021-12-15T14:01:23.413637Z","shell.execute_reply.started":"2021-12-15T14:01:23.074659Z","shell.execute_reply":"2021-12-15T14:01:23.412866Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"And now let’s create a simple anomaly detection system with the help of the moving average. Unfortunately, in this particular series everything is more or less normal, so we’ll intentionally make one of the values abnormal in the dataframe ads_anomaly","metadata":{}},{"cell_type":"code","source":"ads_anomaly = ads.copy()\nads_anomaly.iloc[-20] = ads_anomaly.iloc[-20] * 0.2 # say we have 80% drop of ads","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:23.415239Z","iopub.execute_input":"2021-12-15T14:01:23.415745Z","iopub.status.idle":"2021-12-15T14:01:23.422161Z","shell.execute_reply.started":"2021-12-15T14:01:23.415704Z","shell.execute_reply":"2021-12-15T14:01:23.421528Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Let’s see, if this simple method can catch the anomaly","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:23.425143Z","iopub.execute_input":"2021-12-15T14:01:23.425836Z","iopub.status.idle":"2021-12-15T14:01:23.776711Z","shell.execute_reply.started":"2021-12-15T14:01:23.425785Z","shell.execute_reply":"2021-12-15T14:01:23.775901Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Neat! What about the second series (with weekly smoothing)?","metadata":{}},{"cell_type":"code","source":"plotMovingAverage(currency, 7, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:23.778235Z","iopub.execute_input":"2021-12-15T14:01:23.778722Z","iopub.status.idle":"2021-12-15T14:01:24.151013Z","shell.execute_reply.started":"2021-12-15T14:01:23.778675Z","shell.execute_reply":"2021-12-15T14:01:24.150241Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Oh no! Here is the downside of our simple approach — it did not catch monthly seasonality in our data and marked almost all 30-day peaks as an anomaly. If you don’t want to have that many false alarms — it’s best to consider more complex models.\n\nWeighted average is a simple modification of the moving average, inside of which observations have different weights summing up to one, usually more recent observations have greater weight.","metadata":{}},{"cell_type":"markdown","source":"### Weighted Average","metadata":{}},{"cell_type":"code","source":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)\n  \nweighted_average(ads, [0.6, 0.3, 0.1])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:24.152495Z","iopub.execute_input":"2021-12-15T14:01:24.152728Z","iopub.status.idle":"2021-12-15T14:01:24.162532Z","shell.execute_reply.started":"2021-12-15T14:01:24.152701Z","shell.execute_reply":"2021-12-15T14:01:24.161736Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Exponential smoothing","metadata":{}},{"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n    \ndef plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:24.163694Z","iopub.execute_input":"2021-12-15T14:01:24.163927Z","iopub.status.idle":"2021-12-15T14:01:24.177872Z","shell.execute_reply.started":"2021-12-15T14:01:24.163900Z","shell.execute_reply":"2021-12-15T14:01:24.177243Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plotExponentialSmoothing(ads.Ads, [0.3, 0.05])\nplotExponentialSmoothing(currency.GEMS_GEMS_SPENT, [0.3, 0.05])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:24.178953Z","iopub.execute_input":"2021-12-15T14:01:24.179518Z","iopub.status.idle":"2021-12-15T14:01:24.731408Z","shell.execute_reply.started":"2021-12-15T14:01:24.179451Z","shell.execute_reply":"2021-12-15T14:01:24.730463Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Double exponential smoothing","metadata":{}},{"cell_type":"code","source":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:24.733311Z","iopub.execute_input":"2021-12-15T14:01:24.733610Z","iopub.status.idle":"2021-12-15T14:01:24.745938Z","shell.execute_reply.started":"2021-12-15T14:01:24.733572Z","shell.execute_reply":"2021-12-15T14:01:24.744977Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plotDoubleExponentialSmoothing(ads.Ads, alphas=[0.9, 0.02], betas=[0.9, 0.02])\nplotDoubleExponentialSmoothing(currency.GEMS_GEMS_SPENT, alphas=[0.9, 0.02], betas=[0.9, 0.02])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:24.747365Z","iopub.execute_input":"2021-12-15T14:01:24.747634Z","iopub.status.idle":"2021-12-15T14:01:25.465232Z","shell.execute_reply.started":"2021-12-15T14:01:24.747603Z","shell.execute_reply":"2021-12-15T14:01:25.464331Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Triple exponential smoothing a.k.a. Holt-Winters","metadata":{}},{"cell_type":"code","source":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:25.466559Z","iopub.execute_input":"2021-12-15T14:01:25.466802Z","iopub.status.idle":"2021-12-15T14:01:25.492321Z","shell.execute_reply.started":"2021-12-15T14:01:25.466772Z","shell.execute_reply":"2021-12-15T14:01:25.491401Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Time series cross validation\nNow, knowing how to set cross-validation, we will find optimal parameters for the Holt-Winters model, recall that we have daily seasonality in ads, hence the slen=24 parameter","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n\ndef timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:25.493574Z","iopub.execute_input":"2021-12-15T14:01:25.493899Z","iopub.status.idle":"2021-12-15T14:01:25.525136Z","shell.execute_reply.started":"2021-12-15T14:01:25.493831Z","shell.execute_reply":"2021-12-15T14:01:25.524415Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"In the Holt-Winters model, as well as in the other models of exponential smoothing, there’s a constraint on how big smoothing parameters could be, each of them is in the range from 0 to 1, therefore to minimize loss function we have to choose an algorithm that supports constraints on model parameters, in our case — Truncated Newton conjugate gradient.","metadata":{}},{"cell_type":"code","source":"%%time\ndata = ads.Ads[:-20] # leave some data for testing\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_squared_log_error), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 50 hours\nmodel = HoltWinters(data, slen = 24, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 50, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:25.526406Z","iopub.execute_input":"2021-12-15T14:01:25.527188Z","iopub.status.idle":"2021-12-15T14:01:27.769801Z","shell.execute_reply.started":"2021-12-15T14:01:25.527154Z","shell.execute_reply":"2021-12-15T14:01:27.768946Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-20, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);\n    \nplotHoltWinters(ads.Ads)\nplotHoltWinters(ads.Ads, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:27.771300Z","iopub.execute_input":"2021-12-15T14:01:27.771641Z","iopub.status.idle":"2021-12-15T14:01:28.549320Z","shell.execute_reply.started":"2021-12-15T14:01:27.771595Z","shell.execute_reply":"2021-12-15T14:01:28.548674Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Judging by the chart, our model was able to successfully approximate the initial time series, catching daily seasonality, overall downwards trend and even some anomalies. If you take a look at the modeled deviation, you can clearly see that the model reacts quite sharply to the changes in the structure of the series but then quickly returns deviation to the normal values, “forgetting” the past. This feature of the model allows us to quickly build anomaly detection systems even for quite noisy series without spending too much time and money on preparing data and training the model.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:28.550501Z","iopub.execute_input":"2021-12-15T14:01:28.551177Z","iopub.status.idle":"2021-12-15T14:01:28.791834Z","shell.execute_reply.started":"2021-12-15T14:01:28.551141Z","shell.execute_reply":"2021-12-15T14:01:28.791263Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"We’ll apply the same algorithm for the second series which, as we know, has trend and 30-day seasonality.","metadata":{}},{"cell_type":"code","source":"%%time\ndata = currency.GEMS_GEMS_SPENT[:-50] \nslen = 30 # 30-day seasonality\n\nx = [0, 0, 0] \n\nopt = minimize(timeseriesCVscore, x0=x, \n               args=(data, mean_absolute_percentage_error, slen), \n               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\nmodel = HoltWinters(data, slen = slen, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 100, scaling_factor = 3)\nmodel.triple_exponential_smoothing()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:28.792735Z","iopub.execute_input":"2021-12-15T14:01:28.793507Z","iopub.status.idle":"2021-12-15T14:01:32.547391Z","shell.execute_reply.started":"2021-12-15T14:01:28.793470Z","shell.execute_reply":"2021-12-15T14:01:32.546650Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"plotHoltWinters(currency.GEMS_GEMS_SPENT)\nplotHoltWinters(currency.GEMS_GEMS_SPENT, plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:32.548531Z","iopub.execute_input":"2021-12-15T14:01:32.548779Z","iopub.status.idle":"2021-12-15T14:01:33.349922Z","shell.execute_reply.started":"2021-12-15T14:01:32.548752Z","shell.execute_reply":"2021-12-15T14:01:33.349376Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Looks quite adequate, model has caught both upwards trend and seasonal spikes and overall fits our values nicely\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:33.351179Z","iopub.execute_input":"2021-12-15T14:01:33.351711Z","iopub.status.idle":"2021-12-15T14:01:33.588167Z","shell.execute_reply.started":"2021-12-15T14:01:33.351674Z","shell.execute_reply":"2021-12-15T14:01:33.587279Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# 2. Econometric approach\n\n### Stationarity","metadata":{}},{"cell_type":"code","source":"white_noise = np.random.normal(size=1000)\nwith plt.style.context('bmh'):  \n    plt.figure(figsize=(15, 5))\n    plt.plot(white_noise)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:33.589507Z","iopub.execute_input":"2021-12-15T14:01:33.589743Z","iopub.status.idle":"2021-12-15T14:01:33.845919Z","shell.execute_reply.started":"2021-12-15T14:01:33.589715Z","shell.execute_reply":"2021-12-15T14:01:33.845286Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"So the process generated by standard normal distribution is stationary and oscillates around 0 with with deviation of 1. Now based on this process we will generate a new one where each next value will depend on the previous one: x(t)=ρ*x(t−1)+e(t)","metadata":{}},{"cell_type":"code","source":"def plotProcess(n_samples=1000, rho=0):\n    x = w = np.random.normal(size=n_samples)\n    for t in range(n_samples):\n        x[t] = rho * x[t-1] + w[t]\n\n    with plt.style.context('bmh'):  \n        plt.figure(figsize=(10, 3))\n        plt.plot(x)\n        plt.title(\"Rho {}\\n Dickey-Fuller p-value: {}\".format(rho, round(sm.tsa.stattools.adfuller(x)[1], 3)))\n        \nfor rho in [0, 0.6, 0.9, 1]:\n    plotProcess(rho=rho)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:33.847192Z","iopub.execute_input":"2021-12-15T14:01:33.847599Z","iopub.status.idle":"2021-12-15T14:01:34.922084Z","shell.execute_reply.started":"2021-12-15T14:01:33.847553Z","shell.execute_reply":"2021-12-15T14:01:34.921108Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"On the first chart you can see the same stationary white noise you’ve seen before. On the second one the value of ρρ increased to 0.6, as a result wider cycles appeared on the chart but overall it is still stationary. The third chart deviates even more from the 0 mean but still oscillates around it. Finally, the value of ρ equal to 1 gives us a random walk process — non-stationary time series.\n\nThis happens because after reaching the critical value the series x(t)=ρ*x(t−1)+e(t) does not return to its mean value. If we subtract x(t−1) from the left and the right side we will get x(t)−x(t−1)=(ρ−1)*x(t−1)+e(t), where the expression on the left is called the first difference. If ρ=1 then the first difference gives us stationary white noise e(t). This fact is the main idea of the Dickey-Fuller test for the stationarity of time series (presence of a unit root). If we can get stationary series from non-stationary using the first difference we call those series integrated of order 1. Null hypothesis of the test — time series is non-stationary, was rejected on the first three charts and was accepted on the last one. We’ve got to say that the first difference is not always enough to get stationary series as the process might be integrated of order d, d > 1 (and have multiple unit roots), in such cases the augmented Dickey-Fuller test is used that checks multiple lags at once.","metadata":{}},{"cell_type":"markdown","source":"### Getting rid of non-stationarity and building SARIMA","metadata":{}},{"cell_type":"code","source":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\n        \ntsplot(ads.Ads, lags=60)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:34.923352Z","iopub.execute_input":"2021-12-15T14:01:34.923678Z","iopub.status.idle":"2021-12-15T14:01:35.865440Z","shell.execute_reply.started":"2021-12-15T14:01:34.923647Z","shell.execute_reply":"2021-12-15T14:01:35.864549Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly, initial series are stationary, Dickey-Fuller test rejected null hypothesis that a unit root is present. Actually, it can be seen on the plot itself — we don’t have a visible trend, so mean is constant, variance is pretty much stable throughout the series. The only thing left is seasonality which we have to deal with before modelling. To do so let’s take “seasonal difference” which means a simple subtraction of series from itself with a lag that equals the seasonal period.\n","metadata":{}},{"cell_type":"code","source":"ads_diff = ads.Ads - ads.Ads.shift(24)\ntsplot(ads_diff[24:], lags=60)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:35.866812Z","iopub.execute_input":"2021-12-15T14:01:35.867509Z","iopub.status.idle":"2021-12-15T14:01:36.593286Z","shell.execute_reply.started":"2021-12-15T14:01:35.867470Z","shell.execute_reply":"2021-12-15T14:01:36.592357Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"That’s better, visible seasonality is gone, however autocorrelation function still has too many significant lags. To remove them we’ll take first differences — subtraction of series from itself with lag 1","metadata":{}},{"cell_type":"code","source":"ads_diff = ads_diff - ads_diff.shift(1)\ntsplot(ads_diff[24+1:], lags=60)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:36.594948Z","iopub.execute_input":"2021-12-15T14:01:36.595277Z","iopub.status.idle":"2021-12-15T14:01:37.322186Z","shell.execute_reply.started":"2021-12-15T14:01:36.595238Z","shell.execute_reply":"2021-12-15T14:01:37.321065Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Perfect! Our series now look like something undescribable, oscillating around zero, Dickey-Fuller indicates that it’s stationary and the number of significant peaks in ACF has dropped. We can finally start modelling!","metadata":{}},{"cell_type":"markdown","source":"### ARIMA-family Crash-Course\nA few words about the model. Letter by letter we’ll build the full name — SARIMA(p,d,q)(P,D,Q,s), Seasonal Autoregression Moving Average model:\n- AR(p) — autoregression model, i.e., regression of the time series onto itself. Basic assumption — current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as p. To determine the initial p you need to have a look at PACF plot — find the biggest significant lag, after which most other lags are becoming not significant.\n\n- MA(q) — moving average model. Without going into detail it models the error of the time series, again the assumption is — current error depends on the previous with some lag, which is referred to as q. Initial value can be found on ACF plot with the same logic.\nLet’s have a small break and combine the first 4 letters:\nAR(p) + MA(q) = ARMA(p,q)\n\nWhat we have here is the Autoregressive–moving-average model! If the series is stationary, it can be approximated with those 4 letters. Shall we continue?\n- I(d)— order of integration. It is simply the number of nonseasonal differences needed for making the series stationary. In our case it’s just 1, because we used first differences.\n\nAdding this letter to four previous gives us ARIMA model which knows how to handle non-stationary data with the help of nonseasonal differences. Awesome, last letter left!\n- S(s) — this letter is responsible for seasonality and equals the season period length of the series\nAfter attaching the last letter we find out that instead of one additional parameter we get three in a row — (P,D,Q)\n- P — order of autoregression for seasonal component of the model, again can be derived from PACF, but this time you need to look at the number of significant lags, which are the multiples of the season period length, for example, if the period equals 24 and looking at PACF we see 24-th and 48-th lags are significant, that means initial P should be 2.\n- Q — same logic, but for the moving average model of the seasonal component, use ACF plot\n- D — order of seasonal integration. Can be equal to 1 or 0, depending on whether seasonal differences were applied or not","metadata":{}},{"cell_type":"code","source":"tsplot(ads_diff[24+1:], lags=60)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:37.323880Z","iopub.execute_input":"2021-12-15T14:01:37.324202Z","iopub.status.idle":"2021-12-15T14:01:38.094673Z","shell.execute_reply.started":"2021-12-15T14:01:37.324161Z","shell.execute_reply":"2021-12-15T14:01:38.093755Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"- p is most probably 4, since it’s the last significant lag on PACF after which most others are becoming not significant.\n- d just equals 1, because we had first differences\n- q should be somewhere around 4 as well as seen on ACF\n- P might be 2, since 24-th and 48-th lags are somewhat significant on PACF\n- D again equals 1 — we performed seasonal differentiation\n- Q is probably 1, 24-th lag on ACF is significant, while 48-th is not","metadata":{}},{"cell_type":"code","source":"# setting initial values and some bounds for them\nps = range(2, 5)\nd=1 \nqs = range(2, 5)\nPs = range(0, 3)\nD=1 \nQs = range(0, 2)\ns = 24 # season length is still 24\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:38.096444Z","iopub.execute_input":"2021-12-15T14:01:38.096765Z","iopub.status.idle":"2021-12-15T14:01:38.106073Z","shell.execute_reply.started":"2021-12-15T14:01:38.096724Z","shell.execute_reply":"2021-12-15T14:01:38.105179Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def optimizeSARIMA(parameters_list, d, D, s):\n    \"\"\"\n        Return dataframe with parameters and corresponding AIC\n        \n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order in ARIMA model\n        D - seasonal integration order \n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table\n    \n# %%time\nresult_table = optimizeSARIMA(parameters_list, d, D, s)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:01:38.107630Z","iopub.execute_input":"2021-12-15T14:01:38.108236Z","iopub.status.idle":"2021-12-15T14:10:14.915698Z","shell.execute_reply.started":"2021-12-15T14:01:38.108191Z","shell.execute_reply":"2021-12-15T14:10:14.914742Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# set the parameters that give the lowest AIC\np, q, P, Q = result_table.parameters[0]\n\nbest_model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(p, d, q), \n                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(best_model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:10:14.917645Z","iopub.execute_input":"2021-12-15T14:10:14.918189Z","iopub.status.idle":"2021-12-15T14:10:19.970671Z","shell.execute_reply.started":"2021-12-15T14:10:14.918136Z","shell.execute_reply":"2021-12-15T14:10:19.967507Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Let’s inspect the residuals of the model","metadata":{}},{"cell_type":"code","source":"tsplot(best_model.resid[24+1:], lags=60)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:10:19.972271Z","iopub.execute_input":"2021-12-15T14:10:19.972649Z","iopub.status.idle":"2021-12-15T14:10:20.675568Z","shell.execute_reply.started":"2021-12-15T14:10:19.972603Z","shell.execute_reply":"2021-12-15T14:10:20.675029Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Well, it’s clear that the residuals are stationary, there are no apparent autocorrelations, let’s make predictions using our model\n","metadata":{}},{"cell_type":"code","source":"def plotSARIMA(series, model, n_steps):\n    \"\"\"\n        Plots model vs predicted values\n        \n        series - dataset with timeseries\n        model - fitted SARIMA model\n        n_steps - number of steps to predict in the future\n        \n    \"\"\"\n    # adding model values\n    data = series.copy()\n    data.columns = ['actual']\n    data['arima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model\n    # due to the differentiating\n    data['arima_model'][:s+d] = np.NaN\n    \n    # forecasting on n_steps forward \n    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n    forecast = data.arima_model.append(forecast)\n    # calculate error, again having shifted on s+d steps from the beginning\n    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])\n\n    plt.figure(figsize=(15, 7))\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    plt.plot(forecast, color='r', label=\"model\")\n    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(data.actual, label=\"actual\")\n    plt.legend()\n    plt.grid(True);\n    \nplotSARIMA(ads, best_model, 50)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:10:20.676850Z","iopub.execute_input":"2021-12-15T14:10:20.677169Z","iopub.status.idle":"2021-12-15T14:10:21.012698Z","shell.execute_reply.started":"2021-12-15T14:10:20.677142Z","shell.execute_reply":"2021-12-15T14:10:21.011715Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Linear (and not quite) models on time series","metadata":{}},{"cell_type":"markdown","source":"### Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"#### Lags of time series\nShifting the series n steps back we get a feature column where the current value of time series is aligned with its value at the time t−n. If we make a 1 lag shift and train a model on that feature, the model will be able to forecast 1 step ahead having observed current state of the series. Increasing the lag, say, up to 6 will allow the model to make predictions 6 steps ahead, however it will use data, observed 6 steps back. If something fundamentally changes the series during that unobserved period, the model will not catch the changes and will return forecasts with big error. So, during the initial lag selection one has to find a balance between the optimal prediction quality and the length of forecasting horizon.","metadata":{}},{"cell_type":"code","source":"# Creating a copy of the initial datagrame to make various transformations \ndata = pd.DataFrame(ads.Ads.copy())\ndata.columns = [\"y\"]\n\n# Adding the lag of the target variable from 6 steps back up to 24\nfor i in range(6, 25):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:10:21.016842Z","iopub.execute_input":"2021-12-15T14:10:21.017074Z","iopub.status.idle":"2021-12-15T14:10:21.034998Z","shell.execute_reply.started":"2021-12-15T14:10:21.017035Z","shell.execute_reply":"2021-12-15T14:10:21.034102Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"#### Train a Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# for time-series cross-validation set 5 folds \ntscv = TimeSeriesSplit(n_splits=5)\n\ndef timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test\n\ndef plotModelResults(model, X_train, X_test, plot_intervals=False, plot_anomalies=False, scale=1.96):\n    \"\"\"\n        Plots modelled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_squared_error\")\n        #mae = cv.mean() * (-1)\n        deviation = np.sqrt(cv.std())\n        \n        lower = prediction - (scale * deviation)\n        upper = prediction + (scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    error = mean_absolute_percentage_error(prediction, y_test)\n    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:25.245722Z","iopub.execute_input":"2021-12-15T14:22:25.246237Z","iopub.status.idle":"2021-12-15T14:22:25.264998Z","shell.execute_reply.started":"2021-12-15T14:22:25.246195Z","shell.execute_reply":"2021-12-15T14:22:25.263813Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"y = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\n# reserve 30% of data for testing\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n\n# machine learning in two lines\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nplotModelResults(X_train=X_train, X_test=X_test, model=lr, plot_intervals=True)\nplotCoefficients(lr)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:26.282248Z","iopub.execute_input":"2021-12-15T14:22:26.283097Z","iopub.status.idle":"2021-12-15T14:22:26.998454Z","shell.execute_reply.started":"2021-12-15T14:22:26.283027Z","shell.execute_reply":"2021-12-15T14:22:26.997552Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Well, simple lags and linear regression gave us predictions that are not that far from SARIMA in quality. There are lot’s of unnecessary features, but we’ll do feature selection a bit later. Now let’s continue engineering!\n\nWe’ll add into our dataset hour, day of the week and boolean for the weekend. To do so we need to transform current dataframe index into datetime format and exctract hour and weekday out of it.","metadata":{}},{"cell_type":"code","source":"data.index = pd.to_datetime(data.index)\ndata[\"hour\"] = data.index.hour\ndata[\"weekday\"] = data.index.weekday\ndata['is_weekend'] = data.weekday.isin([5,6])*1","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:47.567949Z","iopub.execute_input":"2021-12-15T14:23:47.568681Z","iopub.status.idle":"2021-12-15T14:23:47.579849Z","shell.execute_reply.started":"2021-12-15T14:23:47.568641Z","shell.execute_reply":"2021-12-15T14:23:47.579004Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title(\"Encoded features\")\ndata.hour.plot()\ndata.weekday.plot()\ndata.is_weekend.plot()\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:24:02.672131Z","iopub.execute_input":"2021-12-15T14:24:02.672445Z","iopub.status.idle":"2021-12-15T14:24:02.949257Z","shell.execute_reply.started":"2021-12-15T14:24:02.672412Z","shell.execute_reply":"2021-12-15T14:24:02.948109Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Since now we have different scales of variables — thousands for lag features and tens for categorical, it’s reasonable to transform them into same scale to continue exploring feature importances and later — regularization.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ny = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:25:00.917494Z","iopub.execute_input":"2021-12-15T14:25:00.917814Z","iopub.status.idle":"2021-12-15T14:25:01.604998Z","shell.execute_reply.started":"2021-12-15T14:25:00.917776Z","shell.execute_reply":"2021-12-15T14:25:01.604088Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Target encoding\nI’d like to add another variant of encoding categorical variables — by mean value. If it’s undesirable to explode dataset by using tons of dummy variables that can lead to the loss of information about the distance, and if they can’t be used as real values because of the conflicts like “0 hours < 23 hours”, then it’s possible to encode a variable with a little bit more interpretable values. Natural idea is to encode with the mean value of the target variable. In our example every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It’s very important to make sure that the mean value is calculated over train set only (or over current cross-validation fold only), so that the model is not aware of the future.I’d like to add another variant of encoding categorical variables — by mean value. If it’s undesirable to explode dataset by using tons of dummy variables that can lead to the loss of information about the distance, and if they can’t be used as real values because of the conflicts like “0 hours < 23 hours”, then it’s possible to encode a variable with a little bit more interpretable values. Natural idea is to encode with the mean value of the target variable. In our example every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It’s very important to make sure that the mean value is calculated over train set only (or over current cross-validation fold only), so that the model is not aware of the future.","metadata":{}},{"cell_type":"code","source":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:25:18.525634Z","iopub.execute_input":"2021-12-15T14:25:18.525904Z","iopub.status.idle":"2021-12-15T14:25:18.530169Z","shell.execute_reply.started":"2021-12-15T14:25:18.525876Z","shell.execute_reply":"2021-12-15T14:25:18.529575Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"average_hour = code_mean(data, 'hour', \"y\")\nplt.figure(figsize=(7, 5))\nplt.title(\"Hour averages\")\npd.DataFrame.from_dict(average_hour, orient='index')[0].plot()\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:25:19.582378Z","iopub.execute_input":"2021-12-15T14:25:19.582652Z","iopub.status.idle":"2021-12-15T14:25:19.819073Z","shell.execute_reply.started":"2021-12-15T14:25:19.582624Z","shell.execute_reply":"2021-12-15T14:25:19.818178Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n    \"\"\"\n        series: pd.DataFrame\n            dataframe with timeseries\n        lag_start: int\n            initial step back in time to slice target variable \n            example - lag_start = 1 means that the model \n                      will see yesterday's values to predict today\n        lag_end: int\n            final step back in time to slice target variable\n            example - lag_end = 4 means that the model \n                      will see up to 4 days back in time to predict today\n        test_size: float\n            size of the test dataset after train/test split as percentage of dataset\n        target_encoding: boolean\n            if True - add target averages to the dataset\n        \n    \"\"\"\n    \n    # copy of the initial dataset\n    data = pd.DataFrame(series.copy())\n    data.columns = [\"y\"]\n    \n    # lags of series\n    for i in range(lag_start, lag_end):\n        data[\"lag_{}\".format(i)] = data.y.shift(i)\n    \n    # datetime features\n    data.index = pd.to_datetime(data.index)\n    data[\"hour\"] = data.index.hour\n    data[\"weekday\"] = data.index.weekday\n    data['is_weekend'] = data.weekday.isin([5,6])*1\n    \n    if target_encoding:\n        # calculate averages on train set only\n        test_index = int(len(data.dropna())*(1-test_size))\n        data['weekday_average'] = list(map(\n            code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n        data[\"hour_average\"] = list(map(\n            code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n\n        # frop encoded variables \n        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n    \n    # train-test split\n    y = data.dropna().y\n    X = data.dropna().drop(['y'], axis=1)\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:29.990949Z","iopub.execute_input":"2021-12-15T14:26:29.991254Z","iopub.status.idle":"2021-12-15T14:26:30.003328Z","shell.execute_reply.started":"2021-12-15T14:26:29.991224Z","shell.execute_reply":"2021-12-15T14:26:30.002411Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lr)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:30.764797Z","iopub.execute_input":"2021-12-15T14:26:30.765099Z","iopub.status.idle":"2021-12-15T14:26:31.476756Z","shell.execute_reply.started":"2021-12-15T14:26:30.765070Z","shell.execute_reply":"2021-12-15T14:26:31.475899Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Here comes overfitting! Hour_average variable was so great on train dataset that the model decided to concentrate all its forces on it - as a result the quality of prediction dropped. This problem can be approached in a variety of ways, for example, we can calculate target encoding not for the whole train set, but for some window instead, that way encodings from the last observed window will probably describe current series state better. Or we can just drop it manually, since we're sure here it makes things only worse.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:10:21.287536Z","iopub.status.idle":"2021-12-15T14:10:21.287822Z","shell.execute_reply.started":"2021-12-15T14:10:21.287669Z","shell.execute_reply":"2021-12-15T14:10:21.287684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regularization and feature selection","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.heatmap(X_train.corr())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:33.991749Z","iopub.execute_input":"2021-12-15T14:27:33.992018Z","iopub.status.idle":"2021-12-15T14:27:34.665754Z","shell.execute_reply.started":"2021-12-15T14:27:33.991990Z","shell.execute_reply":"2021-12-15T14:27:34.664911Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV, RidgeCV\n\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:53.650877Z","iopub.execute_input":"2021-12-15T14:27:53.651194Z","iopub.status.idle":"2021-12-15T14:27:54.531545Z","shell.execute_reply.started":"2021-12-15T14:27:53.651158Z","shell.execute_reply":"2021-12-15T14:27:54.530535Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lasso)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:28:21.456080Z","iopub.execute_input":"2021-12-15T14:28:21.456386Z","iopub.status.idle":"2021-12-15T14:28:22.751609Z","shell.execute_reply.started":"2021-12-15T14:28:21.456352Z","shell.execute_reply":"2021-12-15T14:28:22.750829Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Lasso regression turned out to be more conservative and removed 23-rd lag from most important features (and also dropped 5 features completely) which only made the quality of prediction better.","metadata":{}},{"cell_type":"markdown","source":"# Boosting","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)\n\nplotModelResults(xgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:31.937323Z","iopub.execute_input":"2021-12-15T14:33:31.937611Z","iopub.status.idle":"2021-12-15T14:33:34.851980Z","shell.execute_reply.started":"2021-12-15T14:33:31.937581Z","shell.execute_reply":"2021-12-15T14:33:34.851104Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the winner! The smallest error on the test set among all the models we’ve tried so far.\n\nYet this victory is decieving and it might not be the brightest idea to fit xgboost as soon as you get your hands over time series data. Generally tree-based models poorly handle trends in data, compared to linear models, so you have to detrend your series first or use some tricks to make the magic happen. Ideally — make the series stationary and then use XGBoost, for example, you can forecast trend separately with a linear model and then add predictions from xgboost to get final forecast.\n\n#### Reference: https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3","metadata":{}}]}